---
title: "RNN: lookback 3 years, predict 1 year ahead"
author: "MFBarajas"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
params:
  pc: TRUE
  user_name: "mbarajas"
  common_name: "atlantic cod"
---

```{r,warning=FALSE,message=FALSE}
# Libraries
library(tidyverse)
library(here)
library(keras)
library(gmRi)

#Set appropriate path to box
if(params$pc == TRUE) {
  # MB setup

  user.name <- params$user_name

  mills_path <- shared.path(os.use = "windows", group = "Mills Lab", folder = NULL)
  
} else {
  mills_path <- shared.path(os.use = "unix", group = "Mills Lab", folder = NULL)
  reticulate::use_condaenv("rkeras2020")
}

#Helper Funs
source(here("code/data_cleanup_funs.R"))
source(here("code/data_reshaping_funs.R"))

#GGplot theme
theme_set(theme_bw())
```

# filter and preprocess data

```{r}
# NEFSC bottom trawl
load(str_c(mills_path, "Data/Survdat_Nye_allseason.RData"))

# ID format
survdat$ID <- format(survdat$ID,scientific=FALSE)

# filter years
dat <- survdat %>% filter(EST_YEAR%in%c(1982:2015))

# filter seasons
dat <- dat %>% filter(SEASON%in%c("SPRING","FALL"))

# filter strata
# offshore strata to include are (starts with 1 ends with 0, so offshore strata number 13 is 1130)
# inshore strata to include are (starts with 3 ends with 0, so inshore strata 61 is 3610)
# strata <- c(1130,1140,1150,1160,1170,1180,1190,1200,1210,1220,1230,1240,1250,1260,1270,1280,
#           1290,1300,1360,1370,1380,1390,1400,3560,3580,3590,3600,3610,3630,3640,3650,3660)

# Stratum Key for filtering specific areas
strata_key <- list(
  "Georges Bank"  = c(1130,1140, 1150, 1160, 1170, 1180, 1190, 
                      1200, 1210, 1220, 1230, 1240, 1250, 3560),
  "Gulf of Maine" = c(1260, 1270, 1280, 1290, 1300, 1360, 1370, 1380, 
                      1390, 1400, 3580, 3590, 3600, 3610, 3630, 3640, 3650, 3660))

# the strata we want
strata <- c(
  strata_key$`Georges Bank`, 
  strata_key$`Gulf of Maine`)

# filter Strata
dat <-dat %>% filter(STRATUM%in%strata)


# Pull Target Species
species_pull <- species_selection(survey_data = dat, common_name = params$common_name)
cod <- species_pull$cod
codtow <- species_pull$codtow

# one row for every unique ID
dat <- dat %>% distinct(ID,.keep_all=TRUE)
```

# three size classes

```{r}
# bin lengths to 3 groups: small <20, medium 20-60, large >60
cod$SIZE <- ifelse(cod$LENGTH < 20, "small", ifelse(cod$LENGTH > 60,"large","medium"))
cod$NUM <- 1

# for each ID, count number of small, medium, and large
x <- cod %>% pivot_wider(id_cols = ID, names_from = SIZE, values_from = NUM, values_fn = list(NUM = sum))

# join with codtow
codtow <- left_join(codtow, x, by="ID")

# remove LENGTH, NUMLEN
codtow <- select(codtow, -c(LENGTH, NUMLEN))

# join tows with cod with tows without cod
x <- left_join(dat, codtow, by="ID")

# select columns to keep
x <- select(x, c(1:29,35:43,74,75,76,77,78,88,89,90))

# if COMNAME is NA, fill-in biomass, abundance, small, medium, large with 0
notcod <- which(is.na(x$COMNAME.y))
x$BIOMASS.y[notcod] <- 0
x$ABUNDANCE.y[notcod] <- 0
x$small[notcod] <- 0
x$medium[notcod] <- 0
x$large[notcod] <- 0

# for tows with cod, fill-in size category abundance NA with 0
x$small[is.na(x$small)] <- 0
x$medium[is.na(x$medium)] <- 0
x$large[is.na(x$large)] <- 0

# proportion of relative abundance/biomass allocated to each size class

#Replace Loop
x <- x %>% 
  mutate(
    nsmall   = ABUNDANCE.y * small / (small + medium + large),
    nmedium  = ABUNDANCE.y * medium / (small + medium + large),
    nlarge   = ABUNDANCE.y * large / (small + medium + large),
    nsmall   = ifelse(is.na(nsmall) == T, 0, nsmall),
    nmedium  = ifelse(is.na(nmedium) == T, 0, nmedium),
    nlarge   = ifelse(is.na(nlarge) == T, 0, nlarge),
    bsmall   = BIOMASS.y * small / (small + medium + large),
    bmedium  = BIOMASS.y * medium / (small + medium + large),
    blarge   = BIOMASS.y * large / (small + medium + large),
    bsmall   = ifelse(is.na(bsmall) == T, 0, bsmall),
    bmedium  = ifelse(is.na(bmedium) == T, 0, bmedium),
    blarge   = ifelse(is.na(blarge) == T, 0, blarge)
  )

```

# area of strata

```{r,message=FALSE}
# bring in strata area
strataarea <- read_csv(str_c(mills_path, "Projects/NSF_CAccel/Data/strata area.csv"), 
                       col_types = cols())

strataarea <- select(strataarea, area, stratum)
colnames(x)[25] <- "stratum"
x <- left_join(x, strataarea, by="stratum")

# column names without .x or .y at the end
colnames(x) <- str_split(string = colnames(x), pattern="[.]", simplify=TRUE)[,1]

# Assign strata to area
# Georges Bank: 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 3560
# Gulf of Maine: 1260 1270 1280 1290 1300 1360 1370 1380 1390 1400 3580 3590 3600 3610 3630 3640 3650 3660
# GBstrata <- c(1130,1140,1150,1160,1170,1180,1190,1200,1210,1220,1230,1240,1250,3560)

# specify prior area columns (statistical area and stratum size area)
colnames(x)[c(30,53)] <- c("STATAREA","STRATUMAREA")

x$AREA <- ifelse(x$stratum %in% strata_key$`Georges Bank`, "GB", "GoM")

# calculate annual area/season/size class mean abundance and biomass within strata
# area: GoM, GB
# season: spring, fall
# size class: small, medium, large
x %>%
  group_by(EST_YEAR,AREA,SEASON,stratum) %>%
  summarise(mnsmall     = mean(nsmall), 
            mnmedium    = mean(nmedium),
            mnlarge     = mean(nlarge),
            mnbsmall    = mean(bsmall),
            mnbmedium   = mean(bmedium),
            mnblarge    = mean(blarge),
            STRATUMAREA = mean(STRATUMAREA)
            ) -> q

# calculate stratum weights by stratum area
q %>%
  group_by(EST_YEAR, AREA, SEASON) %>%
  mutate(weight = STRATUMAREA / (sum(STRATUMAREA))
         ) -> q

# calculate annual area/season/size class mean abundance and biomass across strata
q %>%
  group_by(EST_YEAR, AREA, SEASON) %>%
  summarise(abundance_small  = weighted.mean(mnsmall, weight),
            abundance_medium = weighted.mean(mnmedium, weight),
            abundance_large  = weighted.mean(mnlarge, weight),
            biomass_small    = weighted.mean(mnbsmall, weight),
            biomass_medium   = weighted.mean(mnbmedium, weight),
            biomass_large    = weighted.mean(mnblarge, weight)
            ) -> p

# long format for plots
p %>%
  pivot_longer(
    cols = 4:9,
    names_to = c("type", "size"),
    names_patter = "(.*)_(.*)",
    values_to = "value"
  ) -> a
```

# survey data

```{r}
# data frame to store stratified survey year i
# column 1 is year
# other columns will be ordered by area (GoM, GB), season (spring, fall), type (abundance, biomass), size classes s/m/l
# 1 + (2 x 2 x 2 x 3) = 25
survdf<-data.frame(matrix(NA,nrow=34,ncol=25))
colnames(survdf)<-c("year","gom_spr_abun_sml","gom_spr_abun_med","gom_spr_abun_lrg",
                           "gom_spr_bio_sml","gom_spr_bio_med","gom_spr_bio_lrg",
                           "gom_fal_abun_sml","gom_fal_abun_med","gom_fal_abun_lrg",
                           "gom_fal_bio_sml","gom_fal_bio_med","gom_fal_bio_lrg",
                           "gb_spr_abun_sml","gb_spr_abun_med","gb_spr_abun_lrg",
                           "gb_spr_bio_sml","gb_spr_bio_med","gb_spr_bio_lrg",
                           "gb_fal_abun_sml","gb_fal_abun_med","gb_fal_abun_lrg",
                           "gb_fal_bio_sml","gb_fal_bio_med","gb_fal_bio_lrg")

# add year
survdf[,1]<-sort(unique(p$EST_YEAR))

# order of rows from p are 4:1 when subset by year
pdf<-as.data.frame(p)
for(i in 1:34){
  survdf[i,2:25]<-as.numeric(unlist(c(subset(pdf,pdf$EST_YEAR==survdf$year[i])[4,4:9],
                                      subset(pdf,pdf$EST_YEAR==survdf$year[i])[3,4:9],
                                      subset(pdf,pdf$EST_YEAR==survdf$year[i])[2,4:9],
                                      subset(pdf,pdf$EST_YEAR==survdf$year[i])[1,4:9])))
}

# reorder columns all abundance together all biomass together
survdf<-survdf[,c(str_which(colnames(survdf),pattern="bio",negate=TRUE),
                  str_which(colnames(survdf),pattern="bio",negate=FALSE))]
```

# catch data

```{r,message=FALSE}
###############################################################################################
# catch data for year i
gomcatch <- read_csv(str_c(mills_path, "Projects/NSF_CAccel/Data/gom_catch_at_age_19.csv"), 
                     col_types = cols()) # ages 1-9+ years 1982-2018
gbcatch <- read_csv(str_c(mills_path, "Projects/NSF_CAccel/Data/gb_catch_at_age_19.csv"), 
                    col_types = cols()) # ages 1-10+ years 1978-2014
# age overlap is ages 1-9+
# year overlap is years 1982-2014

# remove + signs in column names
colnames(gomcatch)[10]<-"age_9plus"
colnames(gbcatch)[11]<-"age_10"

# for GB, combine age 9 and 10+
gbcatch<-gbcatch%>%mutate(age_9plus=age_9+age_10)
gbcatch<-select(gbcatch,-c(age_9,age_10))

# remove years prior to 1982 for GB
gbcatch<-gbcatch%>%filter(year>=1982)

# remove years after 2014 for GoM
gomcatch<-gomcatch%>%filter(year<=2014)

# rename columns to specify GoM or GB
# catch = c, r1 = GoM, r2 = GB
colnames(gomcatch)[2:10]<-paste("c_r1_",colnames(gomcatch)[2:10],sep="")
colnames(gbcatch)[2:10]<-paste("c_r2_",colnames(gbcatch)[2:10],sep="")
catch<-left_join(gomcatch,gbcatch,by="year")
```

# SST data

```{r,message=FALSE}
###############################################################################################
# Regional SST for year i
SST_GB <- read_csv(str_c(mills_path, "Projects/NSF_CAccel/Data/SSTdata_GB.csv"),
                 col_names = c("year","yranom_gb","year2","m1_gb","m2_gb","m3_gb","m4_gb","m5_gb",
                               "m6_gb","m7_gb","m8_gb","m9_gb","m10_gb","m11_gb","m12_gb", "year3",
                               "m1anom_gb","m2anom_gb","m3anom_gb","m4anom_gb","m5anom_gb","m6anom_gb",
                               "m7anom_gb","m8anom_gb","m9anom_gb","m10anom_gb","m11anom_gb","m12anom_gb"),
                 col_types = cols())

SST_GoM <- read_csv(str_c(mills_path, "Projects/NSF_CAccel/Data/SSTdata_GOM.csv"),
                  col_names=c("year","yranom_gom","year2","m1_gom","m2_gom","m3_gom","m4_gom","m5_gom",
                              "m6_gom","m7_gom","m8_gom","m9_gom","m10_gom","m11_gom","m12_gom","year3",
                              "m1anom_gom","m2anom_gom","m3anom_gom","m4anom_gom","m5anom_gom","m6anom_gom",
                              "m7anom_gom","m8anom_gom","m9anom_gom","m10anom_gom","m11anom_gom","m12anom_gom"),
                  col_types = cols())

# remove extra year columns
SST_GB<-select(SST_GB,-c(year2,year3))
SST_GoM<-select(SST_GoM,-c(year2,year3))

# join data frames together
SST<-left_join(SST_GoM,SST_GB,by="year")
```

# combine survey, catch, and sst data together

```{r,message=FALSE}

# annual stratified mean survey abundance by area/season/size 1982:2015
survey<-survdf[,1:13]

# annual catch age-1 to age-9+ by area 1982:2014
#catch

# annual temperature anomaly by region 1982:2019
# monthly temperature anomaly by region 1982:2019
# monthly average temperature by region 1982:2019
SST<-SST[-1,]

# remove some objects
rm(a,cod,codtow,dat,gbcatch,gomcatch,p,pdf,q,SST_GB,SST_GoM,species_pull,strata_key,
   strataarea,survdat,survdf,x,i,notcod,strata)

# combine
alldata<-inner_join(survey,catch,by="year")
alldata<-inner_join(alldata,SST,by="year")

rm(survey,catch,SST)

# don't include SST anomaly
dat<-alldata[,-c(32,45:57,70:81)]
```

# timestep at season level

```{r}

# spring columns
spr<-c(1,2:4,8:10,14:31,32:37,44:49)

# fall columns
fal<-c(1,5:7,11:13,14:31,38:43,50:55)

# empty data frame
seasondat<-data.frame(matrix(NaN,nrow=1,ncol=length(spr)))

# column names
colnames(seasondat)<-c("year","gom_sml","gom_med","gom_lrg","gb_sml","gb_med","gb_lrg",
                       "c_r1_age_1","c_r1_age_2","c_r1_age_3","c_r1_age_4","c_r1_age_5","c_r1_age_6",
                       "c_r1_age_7","c_r1_age_8","c_r1_age_9plus",
                       "c_r2_age_1","c_r2_age_2","c_r2_age_3","c_r2_age_4","c_r2_age_5","c_r2_age_6",
                       "c_r2_age_7","c_r2_age_8","c_r2_age_9plus",
                       "m1_gom","m2_gom","m3_gom","m4_gom","m5_gom","m6_gom",
                       "m1_gb","m2_gb","m3_gb","m4_gb","m5_gb","m6_gb")

# stack seasons
for(i in 1:nrow(dat)){seasondat<-rbind(seasondat,as.numeric(dat[i,spr]),as.numeric(dat[i,fal]))}

# remove empty row
seasondat<-seasondat[-1,]
row.names(seasondat)<-NULL

# one-hot spring fall
#seasondat$isSpring<-rep(c(1,0),nrow(dat))
#seasondat$isFall<-rep(c(0,1),nrow(dat))

# timestep snapshots every june and december
timestp<-cbind(paste(c(1982:2014),"-06-30",sep=""),paste(c(1982:2014),"-12-31",sep=""))
x<-c()
for(i in 1:33){
  x<-c(x,timestp[i,1],timestp[i,2])
}
seasondat$Date<-as.Date(x)

# reorder columns
#seasondat<-seasondat[,c(40,2:7,38,39,8:37)]
seasondat<-seasondat[,c(38,2:7,8:37)]
```

# moving block sub sampling

```{r}
# data frame
fish<-data.frame(seasondat)

# model parameters
max_len<-6 # look back 6 timesteps

# get vector of starting places, overlap by 6
start_indexes<-seq(1,nrow(fish)-(max_len),by=1)

# training rows, pick even number to end on a fall timestep
training_index<-1:52

# scale all data with stats from only the training data
mean<-apply(fish[training_index,-1],2,mean)
std<-apply(fish[training_index,-1],2,sd)
fish[,2:ncol(fish)]<-scale(fish[,-1],center=mean,scale=std)

# empty matrix
fish_cube<-array(NaN,c(length(start_indexes),max_len+1,ncol(fish)-1))

# fill cube with data sequences
for(j in 1:(ncol(fish)-1)){
  for(i in 1:nrow(fish_cube)){
    fish_cube[i,,j]<-fish[start_indexes[i]:(start_indexes[i]+max_len),j+1]
  }
}
```

# split data into training and testing sets

```{r}
# trim off prediction timestep
X<-fish_cube[,-ncol(fish_cube),]

# select label
label<-1:6
LABELS<-fish_cube[,ncol(fish_cube),label]

# training
features_train<-array(X[training_index,,],dim=c(length(training_index),max_len,dim(fish_cube)[3]))
labels_train<-LABELS[training_index,]

# testing
features_test<-array(X[-training_index,,],dim=c(dim(LABELS)[1]-length(training_index),max_len,dim(fish_cube)[3]))
labels_test<-LABELS[-training_index,]
```

# build and train

```{r,message=FALSE}
# input layer
inputs <- layer_input(shape = dim(features_train)[2:3])

# hidden layers, long short term memory
hidden<- inputs %>% 
  layer_lstm(units=dim(features_train)[3],dropout=0.1,recurrent_dropout=0.5,
             kernel_initializer="he_normal",bias_initializer="he_uniform",return_sequences=TRUE)%>%
  layer_lstm(units=dim(features_train)[3],dropout=0.1,recurrent_dropout=0.5,
             kernel_initializer="he_normal",bias_initializer="he_uniform",activation="relu")

# output for small size class
gom_sml_out <- hidden %>% layer_dense(units=1,name="gom_sml")

# output for medium size class
gom_med_out <- hidden %>% layer_dense(units=1,name="gom_med")

# output for large size class
gom_lrg_out <- hidden %>% layer_dense(units=1,name="gom_lrg")

# output for small size class
gb_sml_out <- hidden %>% layer_dense(units=1,name="gb_sml")

# output for medium size class
gb_med_out <- hidden %>% layer_dense(units=1,name="gb_med")

# output for large size class
gb_lrg_out <- hidden %>% layer_dense(units=1,name="gb_lrg")

# create model
model <- keras_model(inputs = inputs, outputs = c(gom_sml_out,
                                                  gom_med_out,
                                                  gom_lrg_out,
                                                  gb_sml_out,
                                                  gb_med_out,
                                                  gb_lrg_out))

# compile
model %>% compile(optimizer = "adam", loss = "mse", metrics = "mse",loss_weights = list(gom_sml=1,
                                                                                        gom_med=1,
                                                                                        gom_lrg=1,
                                                                                        gb_sml=1,
                                                                                        gb_med=1,
                                                                                        gb_lrg=1))

# summary
summary(model)

# train
learningcurves <- model %>% fit(x = features_train,  y=list(gom_sml=labels_train[,1],
                                                            gom_med=labels_train[,2],
                                                            gom_lrg=labels_train[,3],
                                                            gb_sml=labels_train[,4],
                                                            gb_med=labels_train[,5],
                                                            gb_lrg=labels_train[,6]),
                                batch_size = 1, epochs = 100, verbose = 1)

# plot learning curves
plot(learningcurves)
```

# predict and plot

```{r}
# model performance on test set
evaluate(model,features_test,y=list(gom_sml=labels_test[,1],
                                    gom_med=labels_test[,2],
                                    gom_lrg=labels_test[,3],
                                    gb_sml=labels_test[,4],
                                    gb_med=labels_test[,5],
                                    gb_lrg=labels_test[,6]),verbose=0)

# make predictions
preds<-model%>%predict(features_test)
preds<-cbind(preds[[1]],preds[[2]],preds[[3]],preds[[4]],preds[[5]],preds[[6]])

# unscale
preds<-t((t(preds)*std[label])+mean[label])
y<-t((t(fish[,(label+1)])*std[label])+mean[label])

# observed
res<-data.frame(year=fish$Date,y)
colnames(res)<-c("Date",paste(names(mean[label]),"obs",sep="_"))

# predicted
predrows<-((nrow(fish)-dim(features_test)[1])+1):nrow(fish)
preds<-data.frame(year=fish$Date[predrows],preds)
colnames(preds)<-c("Date",paste(names(mean[label]),"pred",sep="_"))

# combine together
res<-left_join(res,preds,"Date")

# long format
res<-res%>%pivot_longer(cols=2:13,names_to=c("area","size","type"),
                        names_pattern="(.*)_(.*)_(.*)",
                        values_to="value")

# plot
res%>%
  ggplot(aes(x=Date,y=value,color=type))+
  geom_line(size=1)+
  facet_grid(factor(area,levels=c("gom","gb"))+factor(size,levels=c("sml","med","lrg"))~.,scales="free_y")
```
